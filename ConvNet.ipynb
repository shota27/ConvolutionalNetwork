{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from layers import Convolution, MaxPooling, ReLU, Affine, SoftmaxWithLoss,im2col,col2im,BatchNormalization,Adam,Dropout,GlobalAveragePooling,LeakyReLU\n",
    "# from optimizer import RMSProp\n",
    "from PIL import Image\n",
    "import os,shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':32, 'filter_size':5, 'pad':1, 'stride':1},\n",
    "                 pool_param={'pool_size':2, 'pad':0, 'stride':2},\n",
    "                 hidden_size=80, output_size=15, weight_init_std=0.01):  #15にした\n",
    "        \"\"\"\n",
    "        input_size : tuple, 入力の配列形状(チャンネル数、画像の高さ、画像の幅)\n",
    "        conv_param : dict, 畳み込みの条件\n",
    "        pool_param : dict, プーリングの条件\n",
    "        hidden_size : int, 隠れ層のノード数\n",
    "        output_size : int, 出力層のノード数\n",
    "        weight_init_std ： float, 重みWを初期化する際に用いる標準偏差\n",
    "        \"\"\"\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        pool_size = pool_param['pool_size']\n",
    "        pool_pad = pool_param['pad']\n",
    "        pool_stride = pool_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size + 2*filter_pad - filter_size) // filter_stride + 1 # 畳み込み後のサイズ(H,W共通)\n",
    "        pool_output_size = (conv_output_size + 2*pool_pad - pool_size) // pool_stride + 1 # プーリング後のサイズ(H,W共通)\n",
    "        \n",
    "        #追加した部分\n",
    "        #Affineレイヤの引数と実際の形状を揃える  1層目の形状を元に2層目の形状を計算するので、1層目の形状計算の部分もある\n",
    "        conv2_output_size=(pool_output_size+2*filter_pad-filter_size)//filter_stride+1     #input_sizeは畳み込み層の入力画像の高さ  入力画像の高さは畳み込みとマックスプーリングのみで変化している  conv1の出力は(元の入力画像の高さ + 2*self.pad - FH) // self.stride + 1 これはconv_output_sizeのこと    convoutputsizeの後にマックスプーリングで変化した後、pooloutput_sizeがconv2の入力画像の高さ\n",
    "        pool_output_size_2=(conv2_output_size + 2*pool_pad - pool_size) // pool_stride + 1 # プーリング後のサイズ(H,W共通)\n",
    "        conv3_output_size=(pool_output_size_2+2*filter_pad-filter_size)//filter_stride+1     #input_sizeは畳み込み層の入力画像の高さ  入力画像の高さは畳み込みとマックスプーリングのみで変化している  conv1の出力は(元の入力画像の高さ + 2*self.pad - FH) // self.stride + 1 これはconv_output_sizeのこと    convoutputsizeの後にマックスプーリングで変化した後、pooloutput_sizeがconv2の入力画像の高さ\n",
    "        pool_output_size_3=(conv3_output_size + 2*pool_pad - pool_size) // pool_stride + 1 \n",
    "        pool_output_pixel = 64 * pool_output_size_3 * pool_output_size_3 # プーリング後のピクセル総数  畳み込み２層なので、filternumから64にする\n",
    "        self.wrong_image_array_list=[]\n",
    "        self.wrong_image_label_list=[]\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        std = weight_init_std\n",
    "        self.params['W1'] = std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size) # W1は畳み込みフィルターの重みになる  畳み込みは重みの形状で特徴量次元が決まる\n",
    "        self.params['b1'] = np.zeros(filter_num) #b1は畳み込みフィルターのバイアスになる\n",
    "        self.params['W2'] = std *  np.random.randn(pool_output_pixel, hidden_size)   \n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = std *  np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        self.params['W6'] = std *  np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b6'] = np.zeros(hidden_size)\n",
    "        self.params['W7'] = std *  np.random.randn(hidden_size, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W4']=std*np.random.randn(64,filter_num,filter_size,filter_size)#(フィルター数, Conv1の出力チャンネル数, フィルターサイズ, フィルターサイズ) \n",
    "        self.params['b4']=np.zeros(64)\n",
    "        self.params['W5']=std*np.random.randn(64,64,filter_size,filter_size)#(フィルター数, Conv1の出力チャンネル数, フィルターサイズ, フィルターサイズ) \n",
    "        self.params['b5']=np.zeros(64)\n",
    "        \n",
    "        self.params['gamma1']=np.ones(filter_num)  #パラメータ辞書に追加しているということなので、gammaは定義されていないため後で引数の部分は定義している\n",
    "        self.params['beta1']=np.zeros(filter_num)\n",
    "        self.params['gamma2']=np.ones(64)\n",
    "        self.params['beta2']=np.zeros(64)\n",
    "        self.params['gamma3']=np.ones(64)\n",
    "        self.params['beta3']=np.zeros(64)\n",
    "        self.params['gamma4']=np.ones(hidden_size)\n",
    "        self.params['beta4']=np.zeros(hidden_size)\n",
    "        self.params['gamma5']=np.ones(hidden_size)\n",
    "        self.params['beta5']=np.zeros(hidden_size)\n",
    "        self.params['gamma6']=np.ones(hidden_size)\n",
    "        self.params['beta6']=np.zeros(hidden_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad']) # W1が畳み込みフィルターの重み, b1が畳み込みフィルターのバイアスになる  stride=1なので形状は同じ\n",
    "        #バッチ正規化\n",
    "        self.layers['BatchNorm_1']=BatchNormalization(self.params['gamma1'],self.params['beta1'])#,moving_mean=self.params['mm1'],moving_var=self.params['mv1']  #バッチ正規化がチャンネルごとなためスケール変数gamma,シフトパラメータbetaは一次元配列\n",
    "        self.layers['ReLU1'] = ReLU()\n",
    "        self.layers['Pool1'] = MaxPooling(pool_h=pool_size, pool_w=pool_size, stride=pool_stride) #マックスプーリングで形状が小さくなる プーリングの条件\n",
    "        \n",
    "        self.layers['Conv2']=Convolution(self.params['W4'],self.params['b4'],\n",
    "                                         conv_param['stride'],conv_param['pad'])\n",
    "        self.layers['BatchNorm_2']=BatchNormalization(self.params['gamma2'],self.params['beta2'])\n",
    "        self.layers['ReLU2']=ReLU()\n",
    "        self.layers['Pool2']=MaxPooling(pool_h=pool_size,pool_w=pool_size,stride=pool_stride)\n",
    "\n",
    "        self.layers['Conv3']=Convolution(self.params['W5'],self.params['b5'],\n",
    "                                         conv_param['stride'],conv_param['pad'])\n",
    "        self.layers['BatchNorm_3']=BatchNormalization(self.params['gamma3'],self.params['beta3'])\n",
    "        self.layers['ReLU3']=ReLU()\n",
    "        self.layers['Pool3']=MaxPooling(pool_h=pool_size,pool_w=pool_size,stride=pool_stride)\n",
    "\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['BatchNorm_4']=BatchNormalization(self.params['gamma4'],self.params['beta4'])\n",
    "        self.layers['ReLU2'] = ReLU()\n",
    "        self.layers['Affine3'] = Affine(self.params['W6'], self.params['b6'])\n",
    "        self.layers['BatchNorm_5']=BatchNormalization(self.params['gamma5'],self.params['beta5'])\n",
    "        self.layers['ReLU4'] = ReLU()\n",
    "        self.layers['Affine4'] = Affine(self.params['W7'], self.params['b7'])\n",
    "        self.layers['BatchNorm_6']=BatchNormalization(self.params['gamma6'],self.params['beta6'])\n",
    "        self.layers['ReLU5'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()#forwardは、yをsoftmax y,tの交差エントロピー\n",
    "    \n",
    "    def predict(self, x,train_flg=False):\n",
    "        # for layer in self.layers.values():\n",
    "        for key,layer in self.layers.items():\n",
    "            if key in ['Dropout' , 'BatchNorm_1', 'BatchNorm_2' ,'BatchNorm_3', 'BatchNorm_4','BatchNorm_5','BatchNorm_6', 'BatchNorm_7' ,'BatchNorm_8']:\n",
    "                print('trainmode')\n",
    "                x = layer.forward(x,train_flg)\n",
    "            else:\n",
    "                x=layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t,train_flg=False):\n",
    "        \"\"\"\n",
    "            x : 入力データ\n",
    "            t : 教師データ\n",
    "        \"\"\"\n",
    "        y = self.predict(x,train_flg)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        acc = 0.0\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]  #バッチサイズ分28*28の要素がある\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]  #バッチサイズ分0と1のワンホットエンコーディングのリストがある\n",
    "            y = self.predict(tx,train_flg=False)  #バッチサイズ分予測し、予測結果はワンホットエンコーディング\n",
    "            y = np.argmax(y, axis=1)  #argmaxでインデックスにする　\n",
    "            acc += np.sum(y == tt) #インデックスの配列をラベルと比較して一致した要素数を合計    tx[(i*batch_size)+yのワンホット配列とttのワンホット配列が一致していないインデックス]             \n",
    "    \n",
    "            if y!=tt:  #誤分類\n",
    "                    img=np.squeeze(tx)\n",
    "                    self.wrong_image_array_list.append(tx)\n",
    "                    self.wrong_image_label_list.append(tt)\n",
    "\n",
    "                    img=(img*255).astype('uint8')#0~1でスケーリングされているため\n",
    "                    #print(type(img),img.shape)\n",
    "                    img = Image.fromarray(img)\n",
    "                    img_converted=img.convert('L')\n",
    "                    img_converted.save(f\"../../2_notebook/wrong/{count}.png\")\n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    #誤分類ファイルとラベルをself.wrong_image_listとlabel_listに保存しているので、\n",
    "    def save_wrong_list(self):\n",
    "        wrong_images=np.array(self.wrong_image_array_list)\n",
    "        wrong_labels=np.array(self.wrong_image_label_list)\n",
    "        np.save('wrong_images.npy',wrong_images)\n",
    "        np.save('wrong_labels.npy',wrong_labels)\n",
    "        print('images and labels have been saved')\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "\n",
    "        \"\"\"勾配を求める（誤差逆伝播法）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師データ\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t,train_flg=True)  #loss内のpredictは本来train_flg=falseだがTrueに設定\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W4'],grads['b4']=self.layers['Conv2'].dW,self.layers['Conv2'].db\n",
    "        grads['W5'],grads['b5']=self.layers['Conv3'].dW,self.layers['Conv3'].db\n",
    "        grads['W6'], grads['b6'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W7'], grads['b7'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "        grads['gamma1'],grads['beta1']=self.layers['BatchNorm_1'].dgamma,self.layers['BatchNorm_1'].dbeta  #BatchNormalization レイヤーによって計算された gamma と beta の勾配を、grads ディクショナリに 'gamma1' と 'beta1' というキーで格納\n",
    "        grads['gamma2'],grads['beta2']=self.layers['BatchNorm_2'].dgamma,self.layers['BatchNorm_2'].dbeta\n",
    "        grads['gamma3'],grads['beta3']=self.layers['BatchNorm_3'].dgamma,self.layers['BatchNorm_3'].dbeta\n",
    "        grads['gamma4'],grads['beta4']=self.layers['BatchNorm_4'].dgamma,self.layers['BatchNorm_4'].dbeta\n",
    "        grads['gamma5'],grads['beta5']=self.layers['BatchNorm_5'].dgamma,self.layers['BatchNorm_5'].dbeta\n",
    "        grads['gamma6'],grads['beta6']=self.layers['BatchNorm_6'].dgamma,self.layers['BatchNorm_6'].dbeta\n",
    "\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_original=np.load(\"../../1_data/train_data.npy\")\n",
    "train_label_original=np.load(\"../../1_data/train_label.npy\")\n",
    "train_data_original = train_data_original.reshape(-1, 1, 28, 28) \n",
    "print(train_data_original.shape)\n",
    "print(train_label_original.shape)\n",
    "\n",
    "train_data=np.load('../../1_data/train_data_image_data_generator.npy')\n",
    "train_label=np.load('../../1_data/train_label_image_data_generator.npy')\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "\n",
    "train_data=np.concatenate([train_data_original[:3000],train_data[:21000]],axis=0)\n",
    "train_label=np.concatenate([train_label_original[:3000],train_label[:21000]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "train_data=train_data.astype(np.float32)\n",
    "train_label=train_label.astype(np.float32)\n",
    "\n",
    "# CNNのオブジェクト生成\n",
    "snet = SimpleConvNet(input_dim=(1, 28, 28), \n",
    "                     conv_param={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                     pool_param={'pool_size':2, 'pad':0, 'stride':2},\n",
    "                     hidden_size=100, output_size=15, weight_init_std=0.01)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, \n",
    "                                                        test_size=0.2,\n",
    "                                                        shuffle=True\n",
    "                                                        )#random_state=1234\n",
    "train = X_train/255\n",
    "test = X_test/255\n",
    "train = train.reshape(-1, 28*28)\n",
    "test = test.reshape(-1, 28*28)\n",
    "train = train.reshape(-1, 1, 28, 28)  #trainはX_train\n",
    "test = test.reshape(-1, 1, 28, 28)   #testはX_test\n",
    "\n",
    "x=train\n",
    "t=y_train  #正解ラベル\n",
    "test_labels=y_test\n",
    "x = x.reshape(-1,1,28,28) # 配列形式の変形\n",
    "epochs =1 \n",
    "batch_size =128\n",
    "# optimizer = RMSProp(lr=0.0001, rho=0.9)\n",
    "optimizer=Adam(lr=0.001)\n",
    "# 繰り返し回数\n",
    "xsize = x.shape[0]\n",
    "iter_num = np.ceil(xsize / batch_size).astype(np.int)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch=%s\"%epoch)\n",
    "    # シャッフル\n",
    "    idx = np.arange(xsize)\n",
    "    np.random.shuffle(idx)\n",
    "    for it in range(iter_num):\n",
    "        \"\"\"\n",
    "        ランダムなミニバッチを順番に取り出す\n",
    "        \"\"\"\n",
    "        print(\"it=\", it)\n",
    "        mask = idx[batch_size*it : batch_size*(it+1)]\n",
    "        # ミニバッチの生成\n",
    "        x_train = x[mask]\n",
    "        t_train = t[mask]\n",
    "        # 勾配の計算 (誤差逆伝播法を用いる) \n",
    "        grads = snet.gradient(x_train, t_train)\n",
    "        # 更新\n",
    "        optimizer.update(snet.params, grads)\n",
    "    ## 学習経過の記録\n",
    "    # 訓練データにおけるloss\n",
    "    train_loss.append(snet.loss(x,  t))\n",
    "    # テストデータにおけるloss\n",
    "    test_loss.append(snet.loss(test, test_labels))#testはX_test\n",
    "    # 訓練データにて精度を確認\n",
    "    train_accuracy.append(snet.accuracy(x, t))#誤分類ファイル保存には、lossではなくaccuracy\n",
    "    # テストデータにて精度を算出\n",
    "    test_accuracy.append(snet.accuracy(test, test_labels))\n",
    "    \n",
    "    if all (x>0.995 for x in [train_accuracy[epoch],train_accuracy[epoch],test_accuracy[epoch],test_accuracy[epoch]]) or  (train_accuracy[epoch]>0.995 and train_accuracy[epoch-1]>0.995 and test_accuracy[epoch]>0.995 and test_accuracy[epoch-1]>0.995):\n",
    "        params={}\n",
    "        params=snet.params\n",
    "        params['mm1']=snet.layers['BatchNorm_1'].moving_mean#moving_mean属性　paramsとして辞書形でpickleに保存\n",
    "        params['mv1']=snet.layers['BatchNorm_1'].moving_var\n",
    "        params['mm2']=snet.layers['BatchNorm_2'].moving_mean\n",
    "        params['mv2']=snet.layers['BatchNorm_2'].moving_var\n",
    "        params['mm3']=snet.layers['BatchNorm_3'].moving_mean\n",
    "        params['mv3']=snet.layers['BatchNorm_3'].moving_var\n",
    "        params['mm4']=snet.layers['BatchNorm_4'].moving_mean\n",
    "        params['mv4']=snet.layers['BatchNorm_4'].moving_var\n",
    "        params['mm5']=snet.layers['BatchNorm_5'].moving_mean\n",
    "        params['mv5']=snet.layers['BatchNorm_5'].moving_var\n",
    "        params['mm6']=snet.layers['BatchNorm_6'].moving_mean\n",
    "        params['mv6']=snet.layers['BatchNorm_6'].moving_var\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossとaccuracyのグラフ化\n",
    "df_log = pd.DataFrame({\"train_loss\":train_loss,\n",
    "             \"test_loss\":test_loss,\n",
    "             \"train_accuracy\":train_accuracy,\n",
    "             \"test_accuracy\":test_accuracy})\n",
    "\n",
    "df_log.plot(style=['r-', 'r--', 'b-', 'b--'])\n",
    "plt.ylim([0,3])\n",
    "plt.ylabel(\"Accuracy or loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=np.load('../../1_data/test_data.npy')\n",
    "test_label=np.load('../../1_data/test_label.npy')\n",
    "test_data = (test_data/255).astype(np.float32)\n",
    "test_data = test_data.reshape(-1, 28*28)\n",
    "test_data = test_data.reshape(-1, 1, 28, 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取り出したパラメータの保存\n",
    "#上のブロックで保存してある\n",
    "params={}\n",
    "params=snet.params\n",
    "params['mm1']=snet.layers['BatchNorm_1'].moving_mean#moving_mean属性　paramsとして辞書形でpickleに保存\n",
    "params['mv1']=snet.layers['BatchNorm_1'].moving_var\n",
    "params['mm2']=snet.layers['BatchNorm_2'].moving_mean\n",
    "params['mv2']=snet.layers['BatchNorm_2'].moving_var\n",
    "params['mm3']=snet.layers['BatchNorm_3'].moving_mean\n",
    "params['mv3']=snet.layers['BatchNorm_3'].moving_var\n",
    "params['mm4']=snet.layers['BatchNorm_4'].moving_mean\n",
    "params['mv4']=snet.layers['BatchNorm_4'].moving_var\n",
    "params['mm5']=snet.layers['BatchNorm_5'].moving_mean\n",
    "params['mv5']=snet.layers['BatchNorm_5'].moving_var\n",
    "params['mm6']=snet.layers['BatchNorm_6'].moving_mean\n",
    "params['mv6']=snet.layers['BatchNorm_6'].moving_var\n",
    "\n",
    "with open(\"../../2_notebook/submit/katakana_model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメーターの読み込み\n",
    "with open(\"katakana_model.pickle\", \"rb\") as f:\n",
    "    #モデル全体\n",
    "    model = pickle.load(f)\n",
    "    model=SimpleConvNet(input_dim=(1, 28, 28), \n",
    "                        conv_param={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                        pool_param={'pool_size':2, 'pad':0, 'stride':2},\n",
    "                        hidden_size=100, output_size=15, weight_init_std=0.01)\n",
    "\n",
    "\n",
    "    model.layers['Conv1'].W = params['W1']\n",
    "    model.layers['Conv1'].b = params['b1']\n",
    "    model.layers['Conv2'].W = params['W4']\n",
    "    model.layers['Conv2'].b = params['b4']\n",
    "    model.layers['Conv3'].W = params['W5']\n",
    "    model.layers['Conv3'].b = params['b5']\n",
    "\n",
    "    model.layers['Affine1'].W = params['W2']\n",
    "    model.layers['Affine1'].b = params['b2']\n",
    "    model.layers['Affine2'].W = params['W3']\n",
    "    model.layers['Affine2'].b = params['b3']\n",
    "    model.layers['Affine3'].W = params['W6']\n",
    "    model.layers['Affine3'].b = params['b6']\n",
    "    model.layers['Affine4'].W = params['W7']\n",
    "    model.layers['Affine4'].b = params['b7']\n",
    "\n",
    "    model.layers['BatchNorm_1'].gamma=params['gamma1']\n",
    "    model.layers['BatchNorm_1'].beta=params['beta1']\n",
    "    model.layers['BatchNorm_1'].moving_mean=params['mm1']#保存してあったmovingmean属性\n",
    "    model.layers['BatchNorm_1'].moving_var=params['mv1']\n",
    "    model.layers['BatchNorm_2'].gamma=params['gamma2']\n",
    "    model.layers['BatchNorm_2'].beta=params['beta2']\n",
    "    model.layers['BatchNorm_2'].moving_mean=params['mm2']\n",
    "    model.layers['BatchNorm_2'].moving_var=params['mv2']\n",
    "    model.layers['BatchNorm_3'].gamma=params['gamma3']\n",
    "    model.layers['BatchNorm_3'].beta=params['beta3']\n",
    "    model.layers['BatchNorm_3'].moving_mean=params['mm3']\n",
    "    model.layers['BatchNorm_3'].moving_var=params['mv3']\n",
    "    model.layers['BatchNorm_4'].gamma=params['gamma4']\n",
    "    model.layers['BatchNorm_4'].beta=params['beta4']\n",
    "    model.layers['BatchNorm_4'].moving_mean=params['mm4']\n",
    "    model.layers['BatchNorm_4'].moving_var=params['mv4']\n",
    "    model.layers['BatchNorm_5'].gamma=params['gamma5']\n",
    "    model.layers['BatchNorm_5'].beta=params['beta5']\n",
    "    model.layers['BatchNorm_5'].moving_mean=params['mm5']\n",
    "    model.layers['BatchNorm_5'].moving_var=params['mv5']\n",
    "    model.layers['BatchNorm_6'].gamma=params['gamma6']\n",
    "    model.layers['BatchNorm_6'].beta=params['beta6']\n",
    "    model.layers['BatchNorm_6'].moving_mean=params['mm6']\n",
    "    model.layers['BatchNorm_6'].moving_var=params['mv6']\n",
    "    \n",
    "\n",
    "    accuracy = model.accuracy(test_data, test_label)  #モデルがtest_dataについて予測を行い、予測と正解ラベルで正解率を計算\n",
    "    loss  = model.loss(test_data, test_label)  #\n",
    "    print(accuracy)#実際にはサーバーに送信\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
